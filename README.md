# Towards Visual Text Grounding of Multimodal Large Language Model

[Towards visual text grounding of multimodal large language model](https://arxiv.org/abs/2504.04974) <br>
Reasoning and Planning for LLMs @ ICLR2025 || ICLR 2025 Workshop SynthData

This is the repo for the TRIG project, which introduces the task of **Text-Rich Image Grounding (TRIG)** for LMMLs. <br>

This repo mainly contains the benchmark data for TRIG and the corresponding minimal evaluation scripts. 

## Contents
- [Overview](#overview)
- [Highlights](#highlights)
- [Install](#install)
- [Data](#data)
- [Evaluation](#evaluation)
- [Citation](#citation)

## Overview

## Highlights

## Install

## Data

## Evaluation

## Citation

Please consider citing our papers if you think our codes, data, or models are useful. Thank you! <br>

```
@article{li2025towards,
  title={Towards visual text grounding of multimodal large language model},
  author={Li, Ming and Zhang, Ruiyi and Chen, Jian and Gu, Jiuxiang and Zhou, Yufan and Dernoncourt, Franck and Zhu, Wanrong and Zhou, Tianyi and Sun, Tong},
  journal={arXiv preprint arXiv:2504.04974},
  year={2025}
}

```
